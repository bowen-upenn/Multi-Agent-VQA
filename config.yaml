datasets:
  dataset: 'vqa-v2' # 'gqa', 'vqa-v2', 'clevr', 'a-okvqa'
  gqa_dataset_split: 'val'
  gqa_images_dir: '/tmp/datasets/gqa/images'
  gqa_val_questions_file: '/tmp/datasets/gqa/val_balanced_questions.json'
  gqa_val_subset_questions_file: '/tmp/datasets/gqa/gqasubset1000.json'
  gqa_test_questions_file: '/tmp/datasets/gqa/testdev_balanced_questions.json'
  vqa_v2_dataset_split: 'rest-val'  # 'val', 'val1000', 'rest-val', 'test-dev', 'test'
  vqa_v2_val_images_dir: '/tmp/datasets/coco/val2014'
  vqa_v2_val_questions_file: '/tmp/datasets/coco/vqa/v2_OpenEnded_mscoco_val2014_questions.json'
  vqa_v2_val_annotations_file: '/tmp/datasets/coco/vqa/v2_mscoco_val2014_annotations.json'
  vqa_v2_val1000_questions_file: '/tmp/datasets/coco/vqa/v2_OpenEnded_mscoco_val2014_1000subset_questions.json'
  vqa_v2_val1000_annotations_file: '/tmp/datasets/coco/vqa/v2_mscoco_val2014_1000subset_annotations.json'
  vqa_v2_rest_val_questions_file: '/tmp/datasets/coco/vqa/v2_OpenEnded_mscoco_rest_val2014_questions.json'
  vqa_v2_rest_val_annotations_file: '/tmp/datasets/coco/vqa/v2_mscoco_rest_val2014_annotations.json'
  vqa_v2_test_images_dir: '/tmp/datasets/coco/test2015'
  vqa_v2_test_questions_file: '/tmp/datasets/coco/vqa/v2_OpenEnded_mscoco_test2015_questions.json'
  vqa_v2_test_dev_questions_file: '/tmp/datasets/coco/vqa/v2_OpenEnded_mscoco_test-dev2015_questions.json'
  vqa_v2_answer_list: '/tmp/datasets/coco/vqa/answer_list.json'
  clevr_dataset_split: 'val'
  clevr_val_questions_file: ''
  clevr_test_questions_file: ''
  clevr_images_dir: ''
  a_okvqa_dataset_split: 'val'
  a_okvqa_val_questions_file: ''
  a_okvqa_test_questions_file: ''
  a_okvqa_images_dir: ''
  synthetic_data_gpt4_dalle3_dataset_dir: '/tmp/datasets/VQA_Synthetic_Dataset/output'
  synthetic_data_gpt4_dalle3_questions_file: '/tmp/datasets/VQA_Synthetic_Dataset/output/questions/output_questions.json'
  synthetic_data_gpt4_dalle3_images_dir: '/tmp/datasets/VQA_Synthetic_Dataset/output/images'
  synthetic_data_gpt4_dalle3_description_dir: '/tmp/datasets/VQA_Synthetic_Dataset/output/image_descriptions'
  synthetic_data_gpt4_dalle3_answer_file: '/tmp/datasets/VQA_Synthetic_Dataset/output/answers/output_answers.json'
  percent_test: 1 #0.2 #0.01
  num_test_data: 10
  use_num_test_data: False

sam:
  GROUNDING_DINO_CONFIG_PATH: "Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py"
  GROUNDING_DINO_CHECKPOINT_PATH: "Grounded-Segment-Anything/groundingdino_swint_ogc.pth"
  SAM_ENCODER_VERSION: "vit_h"
  SAM_CHECKPOINT_PATH: "Grounded-Segment-Anything/sam_vit_h_4b8939.pth"
  BOX_THRESHOLD: 0.25
  TEXT_THRESHOLD: 0.25
  NMS_THRESHOLD: 0.8
  min_mask_region_area: 1024
  pred_iou_thresh: 0.9
  stability_score_thresh: 0.98
dino:
  GROUNDING_DINO_CONFIG_PATH: "Grounded-Segment-Anything/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py"
  GROUNDING_DINO_CHECKPOINT_PATH: "Grounded-Segment-Anything/groundingdino_swint_ogc.pth"
  BOX_THRESHOLD: 0.35
  TEXT_THRESHOLD: 0.25
llm:
  llm_model: 'gpt-4-turbo'
vlm:
  vlm_model: 'gpt-4-turbo' # 'gpt-4o', 'gpt-4-turbo' ('gpt-4-turbo-2024-04-09')
  min_bbox_size: 32
inference:
  verbose: False
  multi_agent: True
  prompt_type: 'baseline' # 'baseline', 'baseline_longer', 'simple', 'cot', 'ps'
  find_nearby_objects: False
  force_multi_agents: False
  nearby_bbox_iou_threshold: 0.5
  print_every: 20
  output_dir: 'outputs/'
  save_output_response: True